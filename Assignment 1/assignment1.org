#+TITLE: Assignment 1: OpenMP
#+AUTHOR: Evan Bradley
#+DATE: 2018-01-31
#+STARTUP: overview

* Setup
The first half of the code is merely devoted to setup. Preprocessor directives
for including libraries and defining a macro for evaluating the maximum of
two integers are written. 
#+BEGIN_SRC c :tangle assignment1.c
/*
 * assignment1.c
 * Author: Evan Bradley
 */
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <sys/timeb.h>
#include <string.h>
#include <omp.h>

#define max(a, b) ((a) > (b) ? (a) : (b))
#+END_SRC
Then a function from matvec.c is included for timing
the runtime of the functions.
#+BEGIN_SRC c :tangle assignment1.c
/* 
 * The following function is taken from matvec.c on the Moodle coursepage.
 */
double read_timer() {
    struct timeb tm;
    ftime(&tm);
    return (double) tm.time + (double) tm.millitm / 1000.0;
}
#+END_SRC


To check the bins for validity, the sum of the difference between each
bin is calculated and returned.
#+BEGIN_SRC c :tangle assignment1.c
int check_bins(int A[], int B[]) {
    int i, sum = 0;

    for (i = 0; i < 10; i++) {
        sum += A[i] - B[i];
    }

    return sum;
}

int check_matrices(int N, int **A, int **B) {
    int sum = 0;

    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            sum += A[i][j] - B[i][j];
        }
    }

    return sum;
}
#+END_SRC

To test the speed of each algorithm, I defined a number of similar functions
that take in the size of the matrix N, the matrix A (as a pointer to an array
of pointers), and the number of threads. The maximum functions simply return a
maximum integer. The histogram functions, on the other hand, take a variable
to hold the count of numbers found in each bin, along with the maximum number
to calculate the bin size.
#+BEGIN_SRC c :tangle assignment1.c
void init(int N, int M, int **A);
void init_1(int N, int M, int **A, int num_threads);
void init_2(int N, int M, int **A, int num_threads);
void init_3(int N, int M, int **A, int num_threads);
int  maximum(int N, int **A);
int  maximum_1(int N, int **A, int num_threads);
int  maximum_2(int N, int **A, int num_threads);
int  maximum_3(int N, int **A, int num_threads);
void hist(int N, int M, int **A, int bins[]);
void hist_1(int N, int M, int **A, int bins[], int num_threads);
void hist_2(int N, int M, int **A, int bins[], int num_threads);
void hist_3(int N, int M, int **A,
                                 int bins[], int num_threads);
#+END_SRC

The main function first declares variables for use in testing, then parses the
command-line arguments, which are the size of the array, size of the max element,
and number of threads, in that order. The rand function is seeded with a specified
value to make the experiments reproducable. To ensure memory issues with stack
variables are not encountered, the matrix A is placed in dynamically-allocated
memory.
#+BEGIN_SRC c :tangle assignment1.c
int main(int argc, char *argv[]) {
    int N, M;
    int num_threads = 5;
    double time_init, time_init_1, time_init_2,
           time_init_3;
    double time_max, time_max_1, time_max_2,
           time_max_3;
    double time_hist, time_hist_1, time_hist_2,
           time_hist_3;
    int max_base, max_1, max_2, max_3;
    if (argc < 4) {
        fprintf(stderr, "Usage: hw1 <n> <m> <t>\n");
        exit(1);
    }
    N = atoi(argv[1]);
    M = atoi(argv[2]);
    num_threads = atoi(argv[3]);
    int** A = (int **) malloc(N * sizeof(int *));
    for (int i = 0; i < N; i++)
         A[i] = (int *) malloc(N * sizeof(int));

    int** A_1 = (int **) malloc(N * sizeof(int *));
    for (int i = 0; i < N; i++)
         A_1[i] = (int *) malloc(N * sizeof(int));

    int** A_2 = (int **) malloc(N * sizeof(int *));
    for (int i = 0; i < N; i++)
         A_2[i] = (int *) malloc(N * sizeof(int));

    int** A_3 = (int **) malloc(N * sizeof(int *));
    for (int i = 0; i < N; i++)
         A_3[i] = (int *) malloc(N * sizeof(int));

    int bins[10] = { };
    int bins_1[10] = { };
    int bins_2[10] = { };
    int bins_3[10] = { };

    srand((1 << 12));
    //printf("Diff (A, A_1): %d\n", check_matrices(N, A, A_1));
#+END_SRC

To time the functions, the timer is first read into a variable, the function
is run, then the difference between the time before and after the function
ran is assigned to the variable.
#+BEGIN_SRC c :tangle assignment1.c
    time_init = read_timer();
    init(N, M, A);
    time_init = (read_timer() - time_init);

    time_init_1 = read_timer();
    init_1(N, M, A_1, num_threads);
    time_init_1 = (read_timer() - time_init_1);

    time_init_2 = read_timer();
    init_2(N, M, A_2, num_threads);
    time_init_2 = (read_timer() - time_init_2);

    time_init_3 = read_timer();
    init_3(N, M, A_3, num_threads);
    time_init_3 = (read_timer() - time_init_3);

    time_max = read_timer();
    max_base = maximum(N, A);
    time_max = (read_timer() - time_max);

    time_max_1 = read_timer();
    max_1 = maximum_1(N, A, num_threads);
    time_max_1 = (read_timer() - time_max_1);

    time_max_2 = read_timer();
    max_2 = maximum_2(N, A, num_threads);
    time_max_2 = (read_timer() - time_max_2);

    time_max_3 = read_timer();
    max_3 = maximum_3(N, A, num_threads);
    time_max_3 = (read_timer() - time_max_3);

    time_hist = read_timer();
    hist(N, M, A, bins);
    time_hist = (read_timer() - time_hist);

    time_hist_1 = read_timer();
    hist_1(N, M, A, bins_1, num_threads);
    time_hist_1 = (read_timer() - time_hist_1);

    time_hist_2 = read_timer();
    hist_2(N, M, A, bins_2, num_threads);
    time_hist_2 = (read_timer() - time_hist_2);

    time_hist_3 = read_timer();
    hist_3(N, M, A, bins_3, num_threads);
    time_hist_3 = (read_timer() - time_hist_3);

#+END_SRC

After all functions have been timed, they are printed into a table for easy
viewing with the time required to run each.
#+BEGIN_SRC c :tangle assignment1.c
    printf("Algorithm\tTime (ms)\n");
    printf("init-base\t%.4f\n", time_init * 1.0e3);
    printf("init-1\t\t%.4f\n", time_init_1 * 1.0e3);
    printf("init-2\t\t%.4f\n", time_init_2 * 1.0e3);
    printf("init-3\t\t%.4f\n", time_init_3 * 1.0e3);
    printf("max-base\t%.4f\n", time_max * 1.0e3);
    printf("max-1\t\t%.4f\n", time_max_1 * 1.0e3);
    printf("max-2\t\t%.4f\n", time_max_2 * 1.0e3);
    printf("max-3\t\t%.4f\n", time_max_3 * 1.0e3);
    printf("hist-base\t%.4f\n", time_hist * 1.0e3);
    printf("hist-1\t\t%.4f\n", time_hist_1 * 1.0e3);
    printf("hist-2\t\t%.4f\n", time_hist_2 * 1.0e3);
    printf("hist-3\t\t%.4f\n", time_hist_3 * 1.0e3);
    //printf("max_base\t max_1\t\t max_2\t max_3\t ");
    //printf("hist_base\t hist_1\t\t hist_2\t hist_3\n");
    /*printf("%.4f\t %.4f\t %.4f\t %.4f\t\t %.4f\t %.4f\t %.4f\t %.4f\n",
           time * 1.0e3, time_1 * 1.0e3, time_2 * 1.0e3,
           time_3 * 1.0e3, time_hist * 1.0e3,
           time_hist_1 * 1.0e3, time_hist_2 * 1.0e3,
           time_hist_3 * 1.0e3);*/
    /*printf("%d\t\t %d\t\t %d\t\t %d\t\t\t %d\t\t %d\t\t %d\t\t %d\n",
           (max_base - max_base), (max_base - max_p), (max_base - max_p_for),
           (max_base - max_p_for_red), check_bins(bins, bins),
           check_bins(bins, bins_1), check_bins(bins, bins_2),
           check_bins(bins, bins_3));*/

    printf("\n");

    printf("max_base max_1\tmax_2\tmax_3\n");
    printf("%d\t %d\t %d\t %d\n", max_base, max_1, max_2, max_3);

    printf("\n");

   printf("hist:\t");
   for (int i = 0; i < 10; i++)
       printf("%d ", bins[i]);
   printf("\n");

   printf("hist_1:\t");
   for (int i = 0; i < 10; i++)
       printf("%d ", bins_1[i]);
   printf("\n");

   printf("hist_2:\t");
   for (int i = 0; i < 10; i++)
       printf("%d ", bins_2[i]);
   printf("\n");

   printf("hist_3:\t");
   for (int i = 0; i < 10; i++)
       printf("%d ", bins_3[i]);
   printf("\n");
#+END_SRC

Finally, the pointers contained by A are free, followed by freeing A
itself and returning. This concludes the main method, with the pertinent
function implementations following it.
#+BEGIN_SRC c :tangle assignment1.c
    for (int i = 0; i < N; i++)
        free(A[i]);

    free(A);

    for (int i = 0; i < N; i++)
        free(A_1[i]);

    free(A_1);

    for (int i = 0; i < N; i++)
        free(A_2[i]);

    free(A_2);

    for (int i = 0; i < N; i++)
        free(A_3[i]);

    free(A_3);

    return 0;
}
#+END_SRC
* Initializing the array
** Serial
   The matrix is initialized by generating random numbers in the range [0, M).
#+BEGIN_SRC c :tangle assignment1.c
void init(int N, int M, int** A) {
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            A[i][j] = rand() % M;
        }
    }
}
#+END_SRC
** Using the parallel directive
   The parallel directive is employed by getting the id of each thread,
   and using that to divide the matrix into chunks to be processed in parallel.

#+BEGIN_SRC c :tangle assignment1.c
void init_1(int N, int M, int** A, int num_threads) {
    #pragma omp parallel num_threads(num_threads)
    {
		    int tid = omp_get_thread_num();
        int Nt0 = tid * N / num_threads;
        int Nt = (tid + 1) * N / num_threads;
        for (int i = Nt0; i < Nt; i++) {
            for (int j = 0; j < N; j++) {
                A[i][j] = rand() % M;
            }
        }
    }
}
#+END_SRC

** Using the for directive
   The for directive is easily applied to the matrix initialization, as OpenMP
   will automatically divide the original array into portions for each thread.

#+BEGIN_SRC c :tangle assignment1.c
void init_2(int N, int M, int **A, int num_threads) {
    #pragma omp parallel for schedule(dynamic) num_threads(num_threads)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
              A[i][j] = rand() % M;
        }
    }
}
#+END_SRC

** Using the for directive with the collapse clause
   The reduction clause cannot be used on pointers (the target data type for
   this function), so the collapse clause was used in its place.

#+BEGIN_SRC c :tangle assignment1.c
void init_3(int N, int M, int **A, int num_threads) {
    #pragma omp parallel for schedule(dynamic) num_threads(num_threads) collapse(2)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
              A[i][j] = rand() % M;
        }
    }
}
#+END_SRC

* Maximum element of the array
  The first task in the assignment was to find the maximum elements, which was
  accomplished by simply looping through the array and testing for the max
  element at each element. This gives a runtime of \(\Theta(n^2)\), which
  is the theoretical lower bound for such a function.

** Serial execution method for finding the max element
   The serial execution method follows the description above: each element of
   the matrix is accessed through two for-loops, testing for the maximum
   along the way.
#+BEGIN_SRC c :tangle assignment1.c
int maximum(int N, int **A) {
    int i, j, max_element = 0;
    for (i = 0; i < N; i++) {
        for (j = 0; j < N; j++) {
            max_element = max(max_element, A[i][j]);
        }
    }

    return max_element;
}
#+END_SRC

** Using the parallel directive
   The version using the parallel directive splits the code into threads that
   each take a portion of the array, searching for the maximum within their
   sub-arrays. After the parallel block, the maximum for each thread is tested
   against the final maximum to find the actual maximum element.
#+BEGIN_SRC c :tangle assignment1.c
int maximum_1(int N, int **A, int num_threads) {
    int max_element = 0;
    int max_elements[num_threads];
    int n;

    for (n = 0; n < num_threads; n++)
        max_elements[n] = 0;

 #pragma omp parallel num_threads(num_threads)
	{
		int tid = omp_get_thread_num();
        int Nt0 = tid * N / num_threads;
        int Nt = (tid + 1) * N / num_threads;
        int i, j;
        for (i = Nt0; i < Nt; i++) {
            for (j = 0; j < N; j++) {
                max_elements[tid] = max(max_elements[tid], A[i][j]);
            }
        }
    }

    for (n = 0; n < num_threads; n++)
        max_element = max(max_element, max_elements[n]);

    return max_element;
}
#+END_SRC

** Using the for directive
   Using the for directive greatly simplifies the code, but also requires a
   reconsideration of how to search the matrix. Instead of per-thread maximums,
   which would require the costly omp_get_thread_num function, a critical
   section is placed at the end of searching through each sub-array. This will
   cause the code to block for the critical sections \(N\) times, which is
   sub-optimal, but as we will see, still results in a speedup.
#+BEGIN_SRC c :tangle assignment1.c
int maximum_2(int N, int **A, int num_threads) {
    int max_element = 0;
    int local_max_element = 0;

    #pragma omp parallel for schedule(dynamic) num_threads(num_threads) \
            firstprivate(local_max_element)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            local_max_element = max(local_max_element, A[i][j]);
        }

        #pragma omp critical
        max_element = max(max_element, local_max_element);
    }

    return max_element;
}
#+END_SRC

** Using the for directive with the reduction clause
   The reduction clause is by far the simplest and fastest of the parallel
   functions. It keeps a local max_element variable for each thread, taking
   the max of those elements at the end of the parallel block. This combines
   the non-blocking nature of the plain parallel directive with the flexibility
   of the for directive, which accounts for this speedup.
#+BEGIN_SRC c :tangle assignment1.c
int maximum_3(int N, int **A, int num_threads) {
    int max_element = 0;

    #pragma omp parallel for schedule(dynamic) \
            reduction(max: max_element) num_threads(num_threads)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            max_element = max(max_element, A[i][j]);
        }
    }

    return max_element;
}
#+END_SRC

* Calculating histogram bins
  Calculating the histogram bins required more sophistication overall, as it
  necessitated tracking an array as opposed to a single variable.

** Serial execution method for calculating the bin counts
   Serial calculation of the bins involved simply looping over the matrix like
   the previous algorithms, but calculating the index of the bin to increment.
   The index is given by the equation \(\frac{n}{M} \cdot 10\) for an element
   \(n\) in the matrix. This is rearranged in the C code to force integer
   division without casting.

#+BEGIN_SRC c :tangle assignment1.c
void hist(int N, int M, int **A, int bins[]) {
    int i, j, idx;

    for (i = 0; i < N; i++) {
        for (j = 0; j < N; j++) {
            bins[(A[i][j]  * 10) / M] += 1;
        }
    }
}
#+END_SRC

** Calculating the bin counts with the parallel directive
   As with the previous parallel-directive algorithm, execution will divide
   into threads, which store the bin counts in local arrays, which take the form
   of the local_bins variable, indexed by the number of the thread. These are
   then summed into the final bin count after the parallel block.

#+BEGIN_SRC c :tangle assignment1.c
void hist_1(int N, int M, int **A, int bins[], int num_threads) {
    int local_bins[num_threads][10];
    for (int i = 0; i < num_threads; i++) {
        for (int j = 0; j < 10; j++) {
            local_bins[i][j] = 0;
        }
    }

 #pragma omp parallel num_threads(num_threads)
    {
        int tid = omp_get_thread_num();
        int Nt0 = tid * N / num_threads;
        int Nt = (tid + 1) * N / num_threads;
        int i, j;

        for (i = Nt0; i < Nt; i++) {
            for (j = 0; j < N; j++) {
                local_bins[tid][(A[i][j]  * 10) / M] += 1;
            }
        }
    }

    for (int i = 0; i < 10; i++) {
        for (int j = 0; j < num_threads; j++) {
            bins[i] += local_bins[j][i];
        }
    }
}
#+END_SRC

** Using the for directive
   The for directive for this algorithm works similar to the algorithm
   using a for directive to calculate the max element. An array of
   local bins for this loop is declared, which are collected before accumulation
   into the final bins inside a critical section.

#+BEGIN_SRC c :tangle assignment1.c
void hist_2(int N, int M, int **A,
                       int bins[], int num_threads) {
    #pragma omp parallel for schedule(dynamic) num_threads(num_threads)
    for (int i = 0; i < N; i++) {
        int local_bins[10] = {  };

        for (int j = 0; j < N; j++) {
            local_bins[(A[i][j]  * 10) / M] += 1;
        }

        #pragma omp critical
        for (int k = 0; k < 10; k++) {
            bins[k] += local_bins[k];
        }
    }
}
#+END_SRC

** Using the for directive with the reduction clause
   Using the reduction clause, the accumulation of the local bins into the final
   bins is implicit, resulting in a decrease in code with a similar runtime.

#+BEGIN_SRC c :tangle assignment1.c
void hist_3(int N, int M, int **A,
                                 int bins[], int num_threads) {
    #pragma omp parallel for schedule(dynamic) \
            reduction(+: bins[:10]) num_threads(num_threads)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            bins[(A[i][j]  * 10) / M] += 1;
        }
    }
}
#+END_SRC

* Experimental Results
  The program was run with \(N = 10, 10^2, 10^4\) and \(M = 100, 1000\) to
  test the runtimes of the algorithms. The code was run on the Yoko server, with
  a Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz processor and 56 cores. The code is
  correct, so no errors were encountered during any executions. The full program output
  has been placed in an included text file, with the maximum element and number of elements
  in each bin for each run. It should be noted that the runtime for
  both algorithms is a function of $N$, so the size of $M$ does not significantly
  affect runtime. Note also that unless otherwise specified, the algorithms using the for
  directive use dynamic scheduling.

** Testing with size N and max M
    Below are the results of testing various matrix sizes ($N$) and maximum
    possible numbers ($M$). The output for each combination of $N$ and $M$ is
    listed as ($N$, $M$). Each of the parallel algorithms were run across $5$
    threads.

\begin{center}
\begin{tabular}{c c c c c c c}
Algorithm &       (10, 10^2) & (10, 10^3) & (10^2, 10^2) & (10^2, 10^3) & (10^4, 10^2) & (10^4, 10^3) \\ \hline
init-base &       0.0000    & 0.0000     & 0.9999     & 0.0000     &  2177.0000    & 1599.9999   \\
init-1    &       0.0000    & 0.0000     & 12.0001    & 3.9999     &  17280.9999   & 17243.0000  \\
init-2    &       0.0000    & 0.0000     & 3.0000     & 3.9999     &  17686.0001   & 18946.0001  \\
init-3    &       0.0000    & 0.0000     & 4.9999     & 6.0000     &  35490.9999   & 36763.0000  \\
max-base  &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  697.0000     & 421.0000    \\
max-1     &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  385.9999     & 347.9998    \\
max-2     &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  105.0000     & 78.0001     \\
max-3     &       0.0000    & 0.0000     & 0.0000     & 0.9999     &  76.9999      & 71.0001     \\
hist-base &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  715.0002     & 730.0000    \\
hist-1    &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  786.9999     & 674.0000    \\
hist-2    &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  170.0001     & 171.0000    \\
hist-3    &       0.0000    & 0.0000     & 0.0000     & 0.0000     &  188.9999     & 182.9998    \\
\end{tabular}
\end{center}

     The algorithms ran too quickly on matrices with size $N = 10$ for the timer
     to accurately measure it. Increasing $N$ to $N = 100$ showed minor results
     for algorithms with poor runtimes, but quicker algorithms still ran too
     quickly to be measured. However increasing $M$ did not increase the runtime
     for the algorithms, displaying the fact that the algorithms should be
     a function of $N$, not $M$. Setting $N = 10^4$ gave the computer enough
     iterations to properly capture the runtime, and this is where the trends
     appear. We can see that the =init-2= and =init-3= functions perform
     considerably worse in parallel than in serial, but the reason for this
     is unclear; it could be the result of poor caching of the matrix, or
     could come as a result of a peculiarity of the server's environment.

     The =max= and =hist= algorithms showed considerable speedup when run in
     parallel. All three parallel =max= functions showed an improvement in
     speed over their serial counterparts, with the for-directive reduction
     clause generally running quicker than a for-directive without it. Simply
     using the for-directive showed a measurable speedup over manual decomposition,
     however. This is likely due to the overhead involved in the =get_omp_thread_num()=
     function.

     The results are more subtle in the =hist= functions, where manual decomposition
     runs at a similar speed to the serial threads (also likely due to the 
     =get_omp_thread_num()= function). However, as with the =max= functions, the
     algorithms using the for-directive are quite fast.

** Increasing the number of threads
   Below are the results with running the algorithms with $N = 10^4$ and
   $M = 10^3$. The first column shows the code running over $5$ threads,
   and the second over $10$ threads.

\begin{center}
\begin{tabular}{c c c}
Algorithm &       5 Threads   & 10 Threads  \\ \hline
init-base &       1599.9999   & 2674.0000   \\
init-1    &       17243.0000  & 20714.9999  \\
init-2    &       18946.0001  & 18810.0002  \\
init-3    &       36763.0000  & 40885.0000  \\
max-base  &       421.0000    & 546.9999    \\
max-1     &       347.9998    & 400.0001    \\
max-2     &       78.0001     & 69.0000     \\
max-3     &       71.0001     & 53.0000     \\
hist-base &       730.0000    & 1250.0000   \\
hist-1    &       674.0000    & 500.0000    \\
hist-2    &       171.0000    & 104.0001    \\
hist-3    &       182.9998    & 108.0000    \\
\end{tabular}
\end{center}

    It can clearly be seen that =max-2=, =max-3=, =hist-1= =hist-2= and =hist-3= all
    benefit from being spread over more threads. =max-1=, however, performs slightly
    worse (but within the margin of error), which could be due to costs incurred
    from higher overhead.
    
** Static scheduling with the for-directive
   The following chart shows the results of using static scheduling with
   the for-directive. The results of $N = 10^4, M = 10^3$ from the above
   chart are duplicated below for comparison.

\begin{center}
\begin{tabular}{c c c c c}
Algorithm &       Dynamic    & Static, 4  & Static, 64 & Static, 128 \\ \hline
init-base &       1599.9999  & 1667.0001   & 1718.0002   & 1928.0000   \\
init-1    &       17243.0000 & 20426.9998  & 18166.9998  & 18278.0001  \\
init-2    &       18946.0001 & 19998.0001  & 16650.0001  & 17130.9998  \\
init-3    &       36763.0000 & 25256.0000  & 20789.0000  & 19288.0001  \\
max-base  &       421.0000   & 531.9998    & 428.0000    & 428.0000    \\
max-1     &       347.9998   & 439.0001    & 346.0000    & 384.0001    \\
max-2     &       78.0001    & 123.9998    & 114.0001    & 101.0001    \\
max-3     &       71.0001    & 112.0002    & 119.0000    & 110.9998    \\
hist-base &       730.0000   & 1083.9999   & 780.0000    & 755.0001    \\
hist-1    &       674.0000   & 957.9999    & 653.0001    & 1036.9999   \\
hist-2    &       171.0000   & 255.0001    & 168.0000    & 207.9999    \\
hist-3    &       182.9998   & 266.0000    & 179.9998    & 212.0001    \\
\end{tabular}
\end{center}

    The chart shows an overall decrease in performance when using static
    scheduling, except with a chunk size of $64$, where =hist-1= appears
    to match the speed it has when using dynamic chunking. As with previous
    tests, the parallel initialization functions perform poorly.

** Conclusions
   Overall, the parallel algorithms using the for directive appear to have the
   most significant improvement over algorithms using only the parallel directive
   and =omp_get_thread_num= to keep track of local data. It is likely that
   =omp_get_thread_num= incurs overhead due to a query of the thread number, and
   that this results in a substantial increase in runtime for instances where
   it is frequently called. The addition of the reduction clause to algorithms
   with the for directive appeared to have minimal impact aside from the cleaner
   code, and in some cases even slowed down the runtime. Static scheduling with
   a fixed chunk size did not appear to have a significant effect on runtime,
   but it is possible that more chunk sizes would need to be tested to determine
   an optimal chunk size. Similarly, more testing may reveal that dynamic scheduling
   (or guided scheduling) are better options. Finally testing showed throughout
   that the upper limit of the array had no significant impact on runtime. While
   it is likely that operating with larger numbers has runtime implications on
   a certain scale, these did not appear with the magnitudes of numbers used in
   these experiments.
